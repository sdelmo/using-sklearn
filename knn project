## import all the libraries

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
sns.set_style('whitegrid')

## import random data

df=pd.read_csv('KNN_Project_Data')

df.head() ## we check all is well

## EDA -- (data is artificial so <shrug> and just do pairplot)

sns.pairplot(data=df,hue='TARGET CLASS')

## We know that learning algos benefit from standardization of data sets.
## Thus we preprocess the data using sklearns preprocessing lib

from sklearn.preprocessing import StandardScaler

## we create a standardscaler object called scaler

scaler=StandardScaler()

## we fit scaler to the features

scaler.fit(df.drop('TARGET CLASS', axis=1))

## we use .transform method on the features

scaled_features=scaler.transform(df.drop('TARGET CLASS',axis=1))

## we convert the scaled features to a dataframe and look at it to make
## everything is running smoothly

df_feat=pd.DataFrame(data=scaled_features,columns=df.columns[:-1])

## Train-test split

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['TARGET CLASS'],
                                                    test_size=0.30)
   
## we use Kneighbors classifer from sklearn
## we created a knn model instance with n_neighbours=1
## we fit that to the training data

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train,y_train)

## Predictions and Evaluations

pred = knn.predict(X_test)
from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y_test,pred))
print(classification_report(y_test,pred))

## To choose an appropiate k value, we elbow method by using a for loop keeping track
## of error rate for each instanc

error_rate = []

for i in range(1,40):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))
    
## we make a plot comparing our error rate and k value

plt.figure(figsize=(14,9))
plt.plot(range(1,40),error_rate,color='green',linestyle='dashed',
marker='o',markerfacecolor='yellow',markersize=10)
plt.ylabel('Error Rate')
plt.xlabel('K value')
plt.title('Error rate vs K value')

## we look at the plot for our wanted k value, which I personally eyeballed
## we retrain with new kvalue

knn = KNeighborsClassifier(n_neighbors=18)
knn.fit(X_train,y_train)
preda = knn.predict(X_test)

print(confusion_matrix(y_test,preda))
print(classification_report(y_test,preda))





